{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4194,
     "status": "ok",
     "timestamp": 1744552297839,
     "user": {
      "displayName": "Maryam Sajedinia",
      "userId": "08562897053091381387"
     },
     "user_tz": -120
    },
    "id": "SgSNmwN2Lj55"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datetime import datetime\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "from torch import optim, nn\n",
    "\n",
    "from torch.nn import init\n",
    "\n",
    "random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1744552432635,
     "user": {
      "displayName": "Maryam Sajedinia",
      "userId": "08562897053091381387"
     },
     "user_tz": -120
    },
    "id": "zgOxmeSdRkRM"
   },
   "outputs": [],
   "source": [
    "path = '/home/maryam/llamaPersonaResp/Original_Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 7963,
     "status": "ok",
     "timestamp": 1744552441123,
     "user": {
      "displayName": "Maryam Sajedinia",
      "userId": "08562897053091381387"
     },
     "user_tz": -120
    },
    "id": "gFgWZOAfq5bF"
   },
   "outputs": [],
   "source": [
    "TrainSet= pd.read_pickle(f\"{path}/final_train.pkl\")\n",
    "DevelopmentSet = pd.read_pickle(f\"{path}/final_dev.pkl\")\n",
    "TestSet = pd.read_pickle(f\"{path}/final_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1744552442601,
     "user": {
      "displayName": "Maryam Sajedinia",
      "userId": "08562897053091381387"
     },
     "user_tz": -120
    },
    "id": "7c80d7gjz2pF",
    "outputId": "435e0827-993c-40c8-b6d1-5b0221d7e016"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21492"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TestSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1744552443554,
     "user": {
      "displayName": "Maryam Sajedinia",
      "userId": "08562897053091381387"
     },
     "user_tz": -120
    },
    "id": "KloEQuIGrVnJ"
   },
   "outputs": [],
   "source": [
    "def get_batch_train(batch_num, batch_size):\n",
    "    start = batch_num * batch_size\n",
    "    return TrainSet[start:start+batch_size]\n",
    "\n",
    "def get_batch_Dev(dev_batch_num, dev_batch_size):\n",
    "    start = dev_batch_num * dev_batch_size\n",
    "    return DevelopmentSet[start:start+dev_batch_size]\n",
    "\n",
    "def indexesFromSentence(sentence, MAX_LENGTH):\n",
    "    inp = sentence.split()\n",
    "    result = list(map(lambda k: vocab.get(k, 1), inp))[-MAX_LENGTH:] #1= oov\n",
    "    if len(result) < MAX_LENGTH:\n",
    "        result = [0]*(MAX_LENGTH - len(result)) + result\n",
    "    return result\n",
    "\n",
    "def process_train(row):\n",
    "    context ,response ,label = row\n",
    "    context = indexesFromSentence(context, MAX_LENGTH)\n",
    "    response = indexesFromSentence(response,MAX_LENGTH)\n",
    "    label = int(label)\n",
    "    return context,response,label\n",
    "\n",
    "def process_dev(row):\n",
    "    context ,response , label = row\n",
    "    context = indexesFromSentence(context, MAX_LENGTH)\n",
    "    responseCandidate = indexesFromSentence(response,MAX_LENGTH)\n",
    "    label = int(label)\n",
    "    return context,responseCandidate,label\n",
    "\n",
    "def load_glove_embeddings(filename=f'{path}/glove.6B.100d.txt'):\n",
    "    lines = open(filename).readlines()\n",
    "    embeddings = {}\n",
    "    for line in lines:\n",
    "        word = line.split()[0]\n",
    "        embedding = list(map(float, line.split()[1:]))\n",
    "        if word in vocab: embeddings[vocab[word]] = embedding\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10166,
     "status": "ok",
     "timestamp": 1744552454574,
     "user": {
      "displayName": "Maryam Sajedinia",
      "userId": "08562897053091381387"
     },
     "user_tz": -120
    },
    "id": "ULbyRY5LsEiq",
    "outputId": "07009185-e3ac-46a6-acba-552159dd874a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 171935 triples\n",
      "Counting words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171935/171935 [00:04<00:00, 37038.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "619658 words in Wiki Dialogue Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PAD_token = 0\n",
    "oov = 1\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {\"<oov>\":1}\n",
    "        self.word2count = {\"<oov>\":1}\n",
    "        self.index2word = {PAD_token: \"PAD\", oov:\"<oov>\" }\n",
    "        self.num_words  = 2  # Count SOS, EOS, PAD\n",
    "    def addUtternace(self, Utterance):\n",
    "        for word in Utterance.split(' '):\n",
    "            self.addWord(word)\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "WikiVoc= Voc('WikiResponseSelection')\n",
    "print(\"Reading %s triples\" % len(TrainSet))\n",
    "print(\"Counting words...\")\n",
    "for sample in tqdm(TrainSet):\n",
    "    WikiVoc.addUtternace(sample[0])\n",
    "    WikiVoc.addUtternace(sample[1])\n",
    "vocab = WikiVoc.word2index\n",
    "print(\"\\n\")\n",
    "print(WikiVoc.num_words, 'words in Wiki Dialogue Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1744552455655,
     "user": {
      "displayName": "Maryam Sajedinia",
      "userId": "08562897053091381387"
     },
     "user_tz": -120
    },
    "id": "sjotxlQWskbp"
   },
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__( self, input_size, hidden_size, vocab_size, num_layers=1,\n",
    "                 dropout=0, bidirectional=True, rnn_type='gru'):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size // self.num_directions\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding = nn.Embedding(vocab_size, input_size, sparse=False, padding_idx=0)\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(input_size, self.hidden_size, num_layers=num_layers,\n",
    "                              dropout=dropout, bidirectional=bidirectional, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(input_size, self.hidden_size, num_layers=num_layers,\n",
    "                               dropout=dropout, bidirectional=bidirectional, batch_first=True)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, inps):\n",
    "        embs = self.embedding(inps)\n",
    "        outputs, hiddens = self.rnn(embs)\n",
    "        return outputs, hiddens\n",
    "\n",
    "    def init_weights(self):\n",
    "        init.orthogonal_(self.rnn.weight_ih_l0)\n",
    "        init.uniform_(self.rnn.weight_hh_l0, a=-0.01, b=0.01)\n",
    "        glove_embeddings = load_glove_embeddings()\n",
    "        embedding_weights = torch.FloatTensor(self.vocab_size, self.input_size)\n",
    "        init.uniform_(embedding_weights, a=-0.25, b=0.25)\n",
    "        for k,v in glove_embeddings.items():\n",
    "            embedding_weights[k] = torch.FloatTensor(v)\n",
    "        embedding_weights[0] = torch.FloatTensor([0]*self.input_size)\n",
    "        del self.embedding.weight\n",
    "        self.embedding.weight = nn.Parameter(embedding_weights)\n",
    "\n",
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super(DualEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        h_size = self.encoder.hidden_size * self.encoder.num_directions\n",
    "        M = torch.FloatTensor(h_size, h_size)\n",
    "        init.normal_(M)\n",
    "        self.M = nn.Parameter( M, requires_grad=True)\n",
    "\n",
    "    def forward(self, contexts, responses):\n",
    "        context_os, context_hs = self.encoder(contexts)\n",
    "        response_os, response_hs = self.encoder(responses)\n",
    "        if self.encoder.rnn_type == 'lstm':\n",
    "            context_hs = context_hs[0]\n",
    "            response_hs = response_hs[0]\n",
    "        results = []\n",
    "        response_encodings = []\n",
    "        h_size = self.encoder.hidden_size * self.encoder.num_directions\n",
    "        for i in range(len(context_hs[0])):\n",
    "            context_h = context_os[i][-1].view(1, h_size)\n",
    "            response_h = response_os[i][-1].view(h_size, 1)\n",
    "            ans = torch.mm(torch.mm(context_h, self.M), response_h)[0][0]\n",
    "            results.append(torch.sigmoid(ans))\n",
    "            response_encodings.append(response_h)\n",
    "        results = torch.stack(results)\n",
    "        return results, response_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1744552456720,
     "user": {
      "displayName": "Maryam Sajedinia",
      "userId": "08562897053091381387"
     },
     "user_tz": -120
    },
    "id": "0pnd-LBlsx_J"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, min_delta = 0, patience = 0):\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.best = np.inf\n",
    "        self.stop_training = False\n",
    "    def on_epoch_end(self, epoch, current_value):\n",
    "        if np.greater(self.best, (current_value - self.min_delta)):\n",
    "            self.best = current_value\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait > self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.stop_training = True\n",
    "        return self.stop_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1744552457534,
     "user": {
      "displayName": "Maryam Sajedinia",
      "userId": "08562897053091381387"
     },
     "user_tz": -120
    },
    "id": "10B1jRqJs12j"
   },
   "outputs": [],
   "source": [
    "def coreConvertor(batch):\n",
    "    cs = []\n",
    "    rs = []\n",
    "    ys = []\n",
    "    for c,r,y in batch:\n",
    "        cs.append(torch.LongTensor(c))\n",
    "        rs.append(torch.LongTensor(r))\n",
    "        ys.append(torch.FloatTensor([y]))\n",
    "    cs = Variable(torch.stack(cs, 0))\n",
    "    rs = Variable(torch.stack(rs, 0))\n",
    "    ys = Variable(torch.stack(ys, 0))\n",
    "    return cs , rs , ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "executionInfo": {
     "elapsed": 1396272,
     "status": "error",
     "timestamp": 1744553854726,
     "user": {
      "displayName": "Maryam Sajedinia",
      "userId": "08562897053091381387"
     },
     "user_tz": -120
    },
    "id": "qvzVJJrts4BT",
    "outputId": "98cd6c7d-c6ad-4813-a2f2-6cad7e24edc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started...\n",
      "Training on Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/671 [01:51<4:07:40, 22.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m batch = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(process_train, batch))\n\u001b[32m     37\u001b[39m cs , rs , ys = coreConvertor(batch)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m y_preds, responses = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m y_preds=y_preds.reshape([batch_size, \u001b[32m1\u001b[39m])\n\u001b[32m     40\u001b[39m loss = loss_fn(y_preds, ys)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llamaPersonaResp/.paper/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llamaPersonaResp/.paper/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mDualEncoder.forward\u001b[39m\u001b[34m(self, contexts, responses)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, contexts, responses):\n\u001b[32m     49\u001b[39m     context_os, context_hs = \u001b[38;5;28mself\u001b[39m.encoder(contexts)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     response_os, response_hs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encoder.rnn_type == \u001b[33m'\u001b[39m\u001b[33mlstm\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     52\u001b[39m         context_hs = context_hs[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llamaPersonaResp/.paper/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llamaPersonaResp/.paper/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mEncoder.forward\u001b[39m\u001b[34m(self, inps)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inps):\n\u001b[32m     23\u001b[39m     embs = \u001b[38;5;28mself\u001b[39m.embedding(inps)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     outputs, hiddens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs, hiddens\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llamaPersonaResp/.paper/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llamaPersonaResp/.paper/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llamaPersonaResp/.paper/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1121\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     result = _VF.lstm(\n\u001b[32m   1137\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1138\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1145\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1146\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 300\n",
    "num_layers = 1\n",
    "dropout = 0\n",
    "input_size= 100\n",
    "hidden_size=300\n",
    "vocab_size=WikiVoc.num_words\n",
    "bidirectional=True\n",
    "rnn_type='lstm'\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "num_batches = int(len(TrainSet) / batch_size)\n",
    "num_epochs = 2\n",
    "dev_batch_size = 128\n",
    "num_dev_batches = int(len(DevelopmentSet) / dev_batch_size)\n",
    "\n",
    "encoder_model = Encoder(input_size, hidden_size, vocab_size , num_layers, dropout, bidirectional, rnn_type)\n",
    "# encoder_model.cuda()\n",
    "\n",
    "model = DualEncoder(encoder_model)\n",
    "# model.cuda()\n",
    "\n",
    "loss_fn = torch.nn.BCELoss() \n",
    "# loss_fn.cuda()\n",
    "\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "all_early_stopping = EarlyStopping(patience = 3)\n",
    "all_early_stopping.stop_training =False\n",
    "print(\"Training Started...\")\n",
    "for epoch in range(1,num_epochs):\n",
    "    if all_early_stopping.stop_training:\n",
    "        print (\"Reached Early Stopping Patience at Epoch {}\".format(epoch))\n",
    "    print (\"Training on Epoch {}\".format(epoch))\n",
    "    random.shuffle(TrainSet)\n",
    "    for batch_num in tqdm(range(num_batches)):\n",
    "        batch = get_batch_train(batch_num, batch_size)\n",
    "        batch = list(map(process_train, batch))\n",
    "        cs , rs , ys = coreConvertor(batch)\n",
    "        y_preds, responses = model(cs, rs)\n",
    "        y_preds=y_preds.reshape([batch_size, 1])\n",
    "        loss = loss_fn(y_preds, ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del loss, batch\n",
    "    print('Evaluating on Developement Set')\n",
    "    LossDevList = []\n",
    "    for dev_batch_num in range(num_dev_batches):\n",
    "        dev = list(map(process_dev, get_batch_Dev(dev_batch_num, dev_batch_size)))\n",
    "        csD , rsD , ysD = coreConvertor(dev)\n",
    "        y_predsD, responsesD = model(csD, rsD)\n",
    "        y_predsD=y_predsD.reshape([dev_batch_size, 1])\n",
    "        lossD = loss_fn(y_predsD, ysD)\n",
    "        LossDevList.append(round(lossD.tolist(),5))\n",
    "    lossOnDev = sum(LossDevList)/len(LossDevList)\n",
    "    print(\"Obtained loss on Development Set after {} epoch: \".format(epoch),round(lossOnDev,5))\n",
    "    all_early_stopping.on_epoch_end(epoch = (epoch + 1), current_value = round(lossOnDev,5))\n",
    "    if all_early_stopping.wait == 0:\n",
    "        bestModel = model\n",
    "        current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "        torch.save(bestModel.state_dict(), f\"{path}/SiameseRNN{rnn_type} {current_time}.pt\")\n",
    "    del lossD , LossDevList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "aborted",
     "timestamp": 1744548698139,
     "user": {
      "displayName": "Maryam Sajedinia",
      "userId": "08562897053091381387"
     },
     "user_tz": -120
    },
    "id": "OioBx_qmtRY6"
   },
   "outputs": [],
   "source": [
    "def get_Test_Batch(num=None):\n",
    "  if num is None: return TestSet\n",
    "    return [random.choice(TestSet) for _ in range(num)]\n",
    "\n",
    "def process_test(row):\n",
    "    context_words, response_words, *distractor_words_list = row\n",
    "    context = indexesFromSentence(context_words, MAX_LENGTH)\n",
    "    response = indexesFromSentence(response_words, MAX_LENGTH)\n",
    "    distractors = [indexesFromSentence(distractor_words, MAX_LENGTH) for distractor_words in distractor_words_list]\n",
    "    return context, response, distractors\n",
    "\n",
    "def Evaluate(model, size=None):\n",
    "    test = list(map(process_test, get_Test_Batch(size)))\n",
    "    count = [0]*10\n",
    "    for e in tqdm(test):\n",
    "        context, response, distractors = e\n",
    "        with torch.no_grad():\n",
    "            cs = Variable(torch.stack([torch.LongTensor(context) for i in range(10)], 0)).cuda()\n",
    "            rs = [torch.LongTensor(response)]\n",
    "            rs += [torch.LongTensor(distractor) for distractor in distractors]\n",
    "            rs = Variable(torch.stack(rs, 0)).cuda()\n",
    "        results, responses = model(cs, rs)\n",
    "        results = [(e.data).cpu().numpy() for e in results]\n",
    "        better_count = sum(1 for val in results[1:] if val >= results[0])`\n",
    "        count[better_count] += 1\n",
    "    return count\n",
    "\n",
    "def Result(model, size=None):\n",
    "    res = Evaluate(model, size)\n",
    "    print(res)\n",
    "    if size==None: size=len(TestSet)\n",
    "    one_in = res[0]/(size)\n",
    "    two_in = sum(res[:2])/(size)\n",
    "    three_in = sum(res[:5])/(size)\n",
    "    return (\"1 in 10 ==> R@1: %0.2f | R@2: %0.2f | R@5: %0.2f\" %(one_in, two_in, three_in))\n",
    "\n",
    "print(Result(model,4000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_tZxtu_N7Al"
   },
   "outputs": [],
   "source": [
    "def get_Test_Batch(num=None):\n",
    "    if num is None: return TestSet\n",
    "    return [random.choice(TestSet) for _ in range(num)]\n",
    "\n",
    "a = get_Test_Batch(len(TestSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 586,
     "status": "ok",
     "timestamp": 1725136932756,
     "user": {
      "displayName": "Maryam Sajedinia",
      "userId": "08562897053091381387"
     },
     "user_tz": -120
    },
    "id": "zGt_ZCWhLDA3",
    "outputId": "637e85ff-1e69-48a6-bcce-2d8e8fbc3e91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23888"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP48xnfE/knlqq96vRs+5UK",
   "gpuType": "T4",
   "mount_file_id": "1zyaoZsVZIBHTljPznuNQO_ECdJu5Pf2I",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
